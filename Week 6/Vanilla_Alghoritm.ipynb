{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6f_OVdfSCnU",
        "outputId": "c135a9e1-96ca-42c2-dca3-5601fe46676e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Definisikan model kebijakan (policy network)\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.softmax(self.fc2(x), dim=-1)\n",
        "\n",
        "# Definisikan baseline model (value network)\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_size):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Fungsi untuk menghitung return\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    returns = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    return returns\n",
        "\n",
        "# Inisialisasi lingkungan, model, dan optimizer\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "policy = PolicyNetwork(state_size, action_size)\n",
        "value = ValueNetwork(state_size)\n",
        "\n",
        "policy_optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
        "value_optimizer = optim.Adam(value.parameters(), lr=0.01)\n",
        "\n",
        "# Algoritma Vanilla Policy Gradient with Baseline\n",
        "num_episodes = 1000\n",
        "gamma = 0.99  # discount factor\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    states, actions, rewards = [], [], []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        action_probs = policy(state_tensor)\n",
        "        action = np.random.choice(action_size, p=action_probs.detach().numpy())\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Simpan data\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Hitung return dan advantage\n",
        "    returns = compute_returns(rewards, gamma)\n",
        "    returns_tensor = torch.FloatTensor(returns)\n",
        "\n",
        "    policy_loss = []\n",
        "    value_loss = []\n",
        "\n",
        "    for t in range(len(states)):\n",
        "        state_tensor = torch.FloatTensor(states[t])\n",
        "        action_taken = actions[t]\n",
        "\n",
        "        # Hitung advantage\n",
        "        value_estimate = value(state_tensor)\n",
        "        advantage = returns_tensor[t] - value_estimate.item()\n",
        "\n",
        "        # Hitung policy gradient\n",
        "        log_prob = torch.log(policy(state_tensor)[action_taken])\n",
        "        policy_loss.append(-log_prob * advantage)  # Menggunakan advantage\n",
        "\n",
        "        # Hitung loss untuk memperbarui baseline (value network)\n",
        "        value_loss.append((value_estimate - returns_tensor[t]) ** 2)\n",
        "\n",
        "    # Update policy\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    # Update baseline (value network)\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "    value_loss.backward()\n",
        "    value_optimizer.step()\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh7ela0tTKsa",
        "outputId": "a6668e16-cc0f-48bb-da40-5f7e9256cf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 18.0\n",
            "Episode 100, Total Reward: 500.0\n",
            "Episode 200, Total Reward: 338.0\n",
            "Episode 300, Total Reward: 482.0\n",
            "Episode 400, Total Reward: 368.0\n",
            "Episode 500, Total Reward: 500.0\n",
            "Episode 600, Total Reward: 500.0\n",
            "Episode 700, Total Reward: 500.0\n",
            "Episode 800, Total Reward: 500.0\n",
            "Episode 900, Total Reward: 500.0\n"
          ]
        }
      ]
    }
  ]
}